"""
åˆ†åŒºç®¡ç† API è·¯ç”±
æä¾›æ•°æ®åº“è¡¨åˆ†åŒºåˆ›å»ºã€æ¸…ç†ã€ç»Ÿè®¡ç­‰åŠŸèƒ½
"""

import logging
from datetime import datetime
from flask import Blueprint, request, jsonify, render_template
from app.utils.time_utils import time_utils
from flask_login import login_required, current_user
from sqlalchemy import and_, or_, func, desc, text
from app.services.partition_management_service import PartitionManagementService
from app.tasks.partition_management_tasks import get_partition_management_status
from app.tasks.database_size_aggregation_tasks import cleanup_old_aggregations
from app.models.instance import Instance
from app.models.database_size_aggregation import DatabaseSizeAggregation
from app.models.database_size_stat import DatabaseSizeStat
from app.utils.decorators import view_required
from app import db

logger = logging.getLogger(__name__)

# åˆ›å»ºè“å›¾
partition_bp = Blueprint('partition', __name__)

# é¡µé¢è·¯ç”±
@partition_bp.route('/', methods=['GET'])
@login_required
@view_required
def partitions():
    """
    åˆ†åŒºç®¡ç†é¡µé¢æˆ–API
    
    å¦‚æœæ˜¯é¡µé¢è¯·æ±‚ï¼ˆæ— æŸ¥è¯¢å‚æ•°ï¼‰ï¼Œè¿”å›HTMLé¡µé¢
    å¦‚æœæ˜¯APIè¯·æ±‚ï¼ˆæœ‰æŸ¥è¯¢å‚æ•°ï¼‰ï¼Œè¿”å›JSONæ•°æ®
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰æŸ¥è¯¢å‚æ•°ï¼Œå¦‚æœæœ‰åˆ™è¿”å›APIæ•°æ®
    if request.args:
        try:
            logger.info("å¼€å§‹è·å–åˆ†åŒºä¿¡æ¯")
            service = PartitionManagementService()
            result = service.get_partition_info()
            logger.info(f"åˆ†åŒºä¿¡æ¯è·å–ç»“æœ: {result}")
            
            if result['success']:
                return jsonify({
                    'success': True,
                    'data': result,
                    'timestamp': time_utils.now().isoformat()
                })
            else:
                logger.error(f"åˆ†åŒºä¿¡æ¯è·å–å¤±è´¥: {result}")
                return jsonify({
                    'success': False,
                    'error': result.get('message', 'è·å–åˆ†åŒºä¿¡æ¯å¤±è´¥'),
                    'details': result
                }), 500
                
        except Exception as e:
            logger.error(f"è·å–åˆ†åŒºä¿¡æ¯æ—¶å‡ºé”™: {str(e)}", exc_info=True)
            return jsonify({
                'success': False,
                'error': f'è·å–åˆ†åŒºä¿¡æ¯å¤±è´¥: {str(e)}',
                'message': 'ç³»ç»Ÿå†…éƒ¨é”™è¯¯'
            }), 500
    else:
        # æ— æŸ¥è¯¢å‚æ•°ï¼Œè¿”å›HTMLé¡µé¢
        return render_template('database_sizes/partitions.html')

# API è·¯ç”±

@partition_bp.route('/status', methods=['GET'])
@login_required
@view_required
def get_partition_status():
    """
    è·å–åˆ†åŒºç®¡ç†çŠ¶æ€
    
    Returns:
        JSON: åˆ†åŒºçŠ¶æ€
    """
    try:
        result = get_partition_management_status()
        
        if result['success']:
            return jsonify({
                'success': True,
                'data': result,
                'timestamp': time_utils.now().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'error': result['message']
            }), 500
            
    except Exception as e:
        logger.error(f"è·å–åˆ†åŒºçŠ¶æ€æ—¶å‡ºé”™: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500


@partition_bp.route('/test', methods=['GET'])
@login_required
@view_required
def test_partition_service():
    """
    æµ‹è¯•åˆ†åŒºç®¡ç†æœåŠ¡ï¼ˆè°ƒè¯•ç”¨ï¼‰
    
    Returns:
        JSON: æµ‹è¯•ç»“æœ
    """
    try:
        logger.info("å¼€å§‹æµ‹è¯•åˆ†åŒºç®¡ç†æœåŠ¡")
        
        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        try:
            db.session.execute(text("SELECT 1"))
            logger.info("æ•°æ®åº“è¿æ¥æ­£å¸¸")
        except Exception as db_error:
            logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(db_error)}")
            return jsonify({
                'success': False,
                'error': f'æ•°æ®åº“è¿æ¥å¤±è´¥: {str(db_error)}'
            }), 500
        
        # æµ‹è¯•åˆ†åŒºç®¡ç†æœåŠ¡
        service = PartitionManagementService()
        result = service.get_partition_info()
        
        return jsonify({
            'success': True,
            'data': result,
            'timestamp': time_utils.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"æµ‹è¯•åˆ†åŒºç®¡ç†æœåŠ¡æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        return jsonify({
            'success': False,
            'error': f'æµ‹è¯•å¤±è´¥: {str(e)}'
        }), 500


@partition_bp.route('/create', methods=['POST'])
@login_required
@view_required
def create_partition():
    """
    åˆ›å»ºåˆ†åŒº
    
    Returns:
        JSON: åˆ›å»ºç»“æœ
    """
    try:
        data = request.get_json()
        partition_date_str = data.get('date')
        
        if not partition_date_str:
            return jsonify({'error': 'ç¼ºå°‘æ—¥æœŸå‚æ•°'}), 400
        
        # è§£ææ—¥æœŸ
        try:
            from datetime import datetime
            partition_date = datetime.strptime(partition_date_str, '%Y-%m-%d').date()
        except ValueError:
            return jsonify({'error': 'æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼'}), 400
        
        service = PartitionManagementService()
        result = service.create_partition(partition_date)
        
        if result['success']:
            return jsonify({
                'success': True,
                'data': result,
                'timestamp': time_utils.now().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'error': result['message']
            }), 500
            
    except Exception as e:
        logger.error(f"åˆ›å»ºåˆ†åŒºæ—¶å‡ºé”™: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500


@partition_bp.route('/cleanup', methods=['POST'])
@login_required
@view_required
def cleanup_partitions():
    """
    æ¸…ç†æ—§åˆ†åŒº
    
    Returns:
        JSON: æ¸…ç†ç»“æœ
    """
    try:
        data = request.get_json() or {}
        retention_months = data.get('retention_months', 12)
        
        service = PartitionManagementService()
        result = service.cleanup_old_partitions(retention_months=retention_months)
        
        if result['success']:
            return jsonify({
                'success': True,
                'data': result,
                'timestamp': time_utils.now().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'error': result['message']
            }), 500
            
    except Exception as e:
        logger.error(f"æ¸…ç†åˆ†åŒºæ—¶å‡ºé”™: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500


@partition_bp.route('/statistics', methods=['GET'])
@login_required
@view_required
def get_partition_statistics():
    """
    è·å–åˆ†åŒºç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        JSON: ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        service = PartitionManagementService()
        result = service.get_partition_statistics()
        
        if result['success']:
            return jsonify({
                'success': True,
                'data': result,
                'timestamp': time_utils.now().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'error': result['message']
            }), 500
            
    except Exception as e:
        logger.error(f"è·å–åˆ†åŒºç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500


@partition_bp.route('/create-future', methods=['POST'])
@login_required
@view_required
def create_future_partitions():
    """
    åˆ›å»ºæœªæ¥åˆ†åŒº
    
    Returns:
        JSON: åˆ›å»ºç»“æœ
    """
    try:
        data = request.get_json() or {}
        months_ahead = data.get('months_ahead', 3)
        
        service = PartitionManagementService()
        result = service.create_future_partitions(months_ahead=months_ahead)
        
        if result['success']:
            return jsonify({
                'success': True,
                'data': result,
                'timestamp': time_utils.now().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'error': result['message']
            }), 500
            
    except Exception as e:
        logger.error(f"åˆ›å»ºæœªæ¥åˆ†åŒºæ—¶å‡ºé”™: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500


@partition_bp.route('/aggregations/latest', methods=['GET'])
@login_required
@view_required
def get_latest_aggregations():
    """
    è·å–æœ€æ–°çš„æ—¥ã€å‘¨ã€æœˆã€å­£åº¦èšåˆæ•°æ®ï¼ˆç”¨äºå¿«é€Ÿæ£€æŸ¥å¼‚å¸¸ï¼‰
    """
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        instance_id = request.args.get('instance_id', type=int)
        db_type = request.args.get('db_type')
        database_name = request.args.get('database_name')
        
        # æ„å»ºæŸ¥è¯¢
        query = DatabaseSizeAggregation.query.join(Instance)
        
        # è¿‡æ»¤æ‰å·²åˆ é™¤çš„æ•°æ®åº“
        latest_stats_subquery = db.session.query(
            DatabaseSizeStat.instance_id,
            DatabaseSizeStat.database_name,
            DatabaseSizeStat.is_deleted,
            func.row_number().over(
                partition_by=[DatabaseSizeStat.instance_id, DatabaseSizeStat.database_name],
                order_by=DatabaseSizeStat.collected_date.desc()
            ).label('rn')
        ).subquery()
        
        active_databases_subquery = db.session.query(
            latest_stats_subquery.c.instance_id,
            latest_stats_subquery.c.database_name
        ).filter(
            and_(
                latest_stats_subquery.c.rn == 1,
                or_(
                    latest_stats_subquery.c.is_deleted == False,
                    latest_stats_subquery.c.is_deleted.is_(None)
                )
            )
        ).subquery()
        
        query = query.join(
            active_databases_subquery,
            and_(
                DatabaseSizeAggregation.instance_id == active_databases_subquery.c.instance_id,
                DatabaseSizeAggregation.database_name == active_databases_subquery.c.database_name
            )
        )
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        if instance_id:
            query = query.filter(DatabaseSizeAggregation.instance_id == instance_id)
        if db_type:
            query = query.filter(Instance.db_type == db_type)
        if database_name:
            query = query.filter(DatabaseSizeAggregation.database_name == database_name)
        
        # è·å–æ¯ç§å‘¨æœŸç±»å‹çš„æœ€æ–°æ•°æ®
        latest_data = {}
        period_types = ['daily', 'weekly', 'monthly', 'quarterly']
        
        for period_type in period_types:
            # è·å–è¯¥å‘¨æœŸç±»å‹çš„æœ€æ–°æ•°æ®
            period_query = query.filter(DatabaseSizeAggregation.period_type == period_type)
            period_query = period_query.order_by(desc(DatabaseSizeAggregation.period_start))
            
            # è·å–æ¯ä¸ªå®ä¾‹-æ•°æ®åº“ç»„åˆçš„æœ€æ–°è®°å½•
            latest_period_data = {}
            for agg in period_query.all():
                key = f"{agg.instance.name}_{agg.database_name}"
                if key not in latest_period_data:
                    latest_period_data[key] = {
                        'instance': {
                            'id': agg.instance.id,
                            'name': agg.instance.name,
                            'db_type': agg.instance.db_type
                        },
                        'database_name': agg.database_name,
                        'period_type': agg.period_type,
                        'period_start': agg.period_start.isoformat(),
                        'period_end': agg.period_end.isoformat(),
                        'max_size_mb': agg.max_size_mb,
                        'min_size_mb': agg.min_size_mb,
                        'avg_size_mb': agg.avg_size_mb,
                        'data_count': agg.data_count,
                        'calculated_at': agg.calculated_at.isoformat() if agg.calculated_at else None
                    }
            
            latest_data[period_type] = list(latest_period_data.values())
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_records = sum(len(data) for data in latest_data.values())
        
        return jsonify({
            'success': True,
            'data': latest_data,
            'total_records': total_records,
            'period_types': period_types,
            'summary': {
                'daily': len(latest_data.get('daily', [])),
                'weekly': len(latest_data.get('weekly', [])),
                'monthly': len(latest_data.get('monthly', [])),
                'quarterly': len(latest_data.get('quarterly', []))
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–æœ€æ–°èšåˆæ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@partition_bp.route('/aggregations/cleanup', methods=['POST'])
@login_required
@view_required
def cleanup_old_aggregations_api():
    """
    æ¸…ç†æ—§çš„èšåˆæ•°æ®
    
    Returns:
        JSON: æ¸…ç†ç»“æœ
    """
    try:
        data = request.get_json() or {}
        retention_days = data.get('retention_days', 365)
        
        result = cleanup_old_aggregations(retention_days=retention_days)
        
        if result['success']:
            return jsonify({
                'success': True,
                'data': result,
                'timestamp': time_utils.now().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'error': result['message']
            }), 500
            
    except Exception as e:
        logger.error(f"æ¸…ç†æ—§èšåˆæ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500


@partition_bp.route('/api/aggregations/summary', methods=['GET'])
@login_required
@view_required
def get_aggregations_summary():
    """
    è·å–èšåˆæ•°æ®ç»Ÿè®¡æ¦‚è§ˆ
    """
    try:
        # è·å–æ¯ç§å‘¨æœŸç±»å‹çš„è®°å½•æ•°
        daily_count = DatabaseSizeAggregation.query.filter(
            DatabaseSizeAggregation.period_type == 'daily'
        ).count()
        
        weekly_count = DatabaseSizeAggregation.query.filter(
            DatabaseSizeAggregation.period_type == 'weekly'
        ).count()
        
        monthly_count = DatabaseSizeAggregation.query.filter(
            DatabaseSizeAggregation.period_type == 'monthly'
        ).count()
        
        quarterly_count = DatabaseSizeAggregation.query.filter(
            DatabaseSizeAggregation.period_type == 'quarterly'
        ).count()
        
        return jsonify({
            'success': True,
            'daily': daily_count,
            'weekly': weekly_count,
            'monthly': monthly_count,
            'quarterly': quarterly_count
        })
        
    except Exception as e:
        logger.error(f"è·å–èšåˆæ•°æ®æ¦‚è§ˆæ—¶å‡ºé”™: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@partition_bp.route('/api/aggregations/core-metrics', methods=['GET'])
@login_required
@view_required
def get_core_aggregation_metrics():
    """
    è·å–æ ¸å¿ƒèšåˆæŒ‡æ ‡æ•°æ®
    è¿”å›4ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼šå®ä¾‹æ•°æ€»é‡ã€æ•°æ®åº“æ•°æ€»é‡ã€å®ä¾‹æ—¥ç»Ÿè®¡æ•°é‡ã€æ•°æ®åº“æ—¥ç»Ÿè®¡æ•°é‡
    """
    try:
        from datetime import date, timedelta
        
        # è·å–æŸ¥è¯¢å‚æ•°
        period_type = request.args.get('period_type', 'daily')
        days = request.args.get('days', 7, type=int)
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = date.today()
        start_date = end_date - timedelta(days=days-1)
        
        # æŸ¥è¯¢æ•°æ®åº“èšåˆæ•°æ®
        db_aggregations = DatabaseSizeAggregation.query.filter(
            DatabaseSizeAggregation.period_type == period_type,
            DatabaseSizeAggregation.period_start >= start_date,
            DatabaseSizeAggregation.period_start <= end_date
        ).all()
        
        # æŸ¥è¯¢å®ä¾‹èšåˆæ•°æ®
        from app.models.instance_size_aggregation import InstanceSizeAggregation
        instance_aggregations = InstanceSizeAggregation.query.filter(
            InstanceSizeAggregation.period_type == period_type,
            InstanceSizeAggregation.period_start >= start_date,
            InstanceSizeAggregation.period_start <= end_date
        ).all()
        
        # æŸ¥è¯¢åŸå§‹ç»Ÿè®¡æ•°æ®
        from app.models.database_size_stat import DatabaseSizeStat
        from app.models.instance_size_stat import InstanceSizeStat
        
        db_stats = DatabaseSizeStat.query.filter(
            DatabaseSizeStat.collected_date >= start_date,
            DatabaseSizeStat.collected_date <= end_date
        ).all()
        
        instance_stats = InstanceSizeStat.query.filter(
            InstanceSizeStat.collected_date >= start_date,
            InstanceSizeStat.collected_date <= end_date
        ).all()
        
        # æŒ‰æ—¥æœŸç»Ÿè®¡4ä¸ªæ ¸å¿ƒæŒ‡æ ‡
        from collections import defaultdict
        daily_metrics = defaultdict(lambda: {
            'instance_count': 0,      # æ¯å¤©é‡‡é›†çš„å®ä¾‹æ•°æ€»é‡ï¼ˆæ—¥ç»Ÿè®¡ï¼‰æˆ–å¹³å‡å€¼ï¼ˆå‘¨/æœˆ/å­£ç»Ÿè®¡ï¼‰
            'database_count': 0,      # æ¯å¤©é‡‡é›†çš„æ•°æ®åº“æ•°æ€»é‡ï¼ˆæ—¥ç»Ÿè®¡ï¼‰æˆ–å¹³å‡å€¼ï¼ˆå‘¨/æœˆ/å­£ç»Ÿè®¡ï¼‰
            'instance_aggregation_count': 0,  # èšåˆç»Ÿè®¡ä¸‹çš„å®ä¾‹ç»Ÿè®¡æ•°é‡
            'database_aggregation_count': 0   # èšåˆç»Ÿè®¡ä¸‹çš„æ•°æ®åº“ç»Ÿè®¡æ•°é‡
        })
        
        # ç»Ÿè®¡åŸå§‹é‡‡é›†æ•°æ®
        for stat in db_stats:
            date_str = stat.collected_date.strftime('%Y-%m-%d')
            daily_metrics[date_str]['database_count'] += 1
        
        for stat in instance_stats:
            date_str = stat.collected_date.strftime('%Y-%m-%d')
            daily_metrics[date_str]['instance_count'] += 1
        
        # ç»Ÿè®¡èšåˆæ•°æ®
        for agg in db_aggregations:
            date_str = agg.period_start.strftime('%Y-%m-%d')
            daily_metrics[date_str]['database_aggregation_count'] += 1
        
        for agg in instance_aggregations:
            date_str = agg.period_start.strftime('%Y-%m-%d')
            daily_metrics[date_str]['instance_aggregation_count'] += 1
        
        # å¯¹äºå‘¨ã€æœˆã€å­£ç»Ÿè®¡ï¼Œéœ€è¦è®¡ç®—å¹³å‡å€¼
        if period_type in ['weekly', 'monthly', 'quarterly']:
            # è®¡ç®—æ¯ä¸ªå‘¨æœŸå†…çš„å¹³å‡å€¼
            period_metrics = defaultdict(lambda: {
                'instance_count': 0,
                'database_count': 0,
                'instance_aggregation_count': 0,
                'database_aggregation_count': 0,
                'days_in_period': 0
            })
            
            # æŒ‰å‘¨æœŸåˆ†ç»„è®¡ç®—å¹³å‡å€¼
            for date_str, metrics in daily_metrics.items():
                period_start = datetime.strptime(date_str, '%Y-%m-%d').date()
                
                if period_type == 'weekly':
                    # è®¡ç®—å‘¨çš„å¼€å§‹æ—¥æœŸï¼ˆå‘¨ä¸€ï¼‰
                    week_start = period_start - timedelta(days=period_start.weekday())
                    period_key = week_start.strftime('%Y-%m-%d')
                elif period_type == 'monthly':
                    # è®¡ç®—æœˆçš„å¼€å§‹æ—¥æœŸ
                    month_start = period_start.replace(day=1)
                    period_key = month_start.strftime('%Y-%m-%d')
                elif period_type == 'quarterly':
                    # è®¡ç®—å­£åº¦çš„å¼€å§‹æ—¥æœŸ
                    quarter_month = ((period_start.month - 1) // 3) * 3 + 1
                    quarter_start = period_start.replace(month=quarter_month, day=1)
                    period_key = quarter_start.strftime('%Y-%m-%d')
                
                period_metrics[period_key]['instance_count'] += metrics['instance_count']
                period_metrics[period_key]['database_count'] += metrics['database_count']
                period_metrics[period_key]['instance_aggregation_count'] += metrics['instance_aggregation_count']
                period_metrics[period_key]['database_aggregation_count'] += metrics['database_aggregation_count']
                period_metrics[period_key]['days_in_period'] += 1
            
            # è®¡ç®—å¹³å‡å€¼å¹¶æ›´æ–°daily_metrics
            daily_metrics = defaultdict(lambda: {
                'instance_count': 0,
                'database_count': 0,
                'instance_aggregation_count': 0,
                'database_aggregation_count': 0
            })
            
            for period_key, metrics in period_metrics.items():
                if metrics['days_in_period'] > 0:
                    daily_metrics[period_key]['instance_count'] = round(metrics['instance_count'] / metrics['days_in_period'], 1)
                    daily_metrics[period_key]['database_count'] = round(metrics['database_count'] / metrics['days_in_period'], 1)
                    daily_metrics[period_key]['instance_aggregation_count'] = metrics['instance_aggregation_count']
                    daily_metrics[period_key]['database_aggregation_count'] = metrics['database_aggregation_count']
        
        # ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
        labels = []
        instance_count_data = []
        database_count_data = []
        instance_aggregation_data = []
        database_aggregation_data = []
        
        current_date = start_date
        while current_date <= end_date:
            date_str = current_date.strftime('%Y-%m-%d')
            labels.append(date_str)
            
            metrics = daily_metrics[date_str]
            instance_count_data.append(metrics['instance_count'])
            database_count_data.append(metrics['database_count'])
            instance_aggregation_data.append(metrics['instance_aggregation_count'])
            database_aggregation_data.append(metrics['database_aggregation_count'])
            
            current_date += timedelta(days=1)
        
        # æ ¹æ®ç»Ÿè®¡å‘¨æœŸç¡®å®šæ ‡ç­¾
        if period_type == 'daily':
            instance_label = 'å®ä¾‹æ•°æ€»é‡'
            database_label = 'æ•°æ®åº“æ•°æ€»é‡'
            instance_agg_label = 'å®ä¾‹æ—¥ç»Ÿè®¡æ•°é‡'
            database_agg_label = 'æ•°æ®åº“æ—¥ç»Ÿè®¡æ•°é‡'
        elif period_type == 'weekly':
            instance_label = 'å®ä¾‹æ•°å¹³å‡å€¼ï¼ˆå‘¨ï¼‰'
            database_label = 'æ•°æ®åº“æ•°å¹³å‡å€¼ï¼ˆå‘¨ï¼‰'
            instance_agg_label = 'å®ä¾‹å‘¨ç»Ÿè®¡æ•°é‡'
            database_agg_label = 'æ•°æ®åº“å‘¨ç»Ÿè®¡æ•°é‡'
        elif period_type == 'monthly':
            instance_label = 'å®ä¾‹æ•°å¹³å‡å€¼ï¼ˆæœˆï¼‰'
            database_label = 'æ•°æ®åº“æ•°å¹³å‡å€¼ï¼ˆæœˆï¼‰'
            instance_agg_label = 'å®ä¾‹æœˆç»Ÿè®¡æ•°é‡'
            database_agg_label = 'æ•°æ®åº“æœˆç»Ÿè®¡æ•°é‡'
        elif period_type == 'quarterly':
            instance_label = 'å®ä¾‹æ•°å¹³å‡å€¼ï¼ˆå­£ï¼‰'
            database_label = 'æ•°æ®åº“æ•°å¹³å‡å€¼ï¼ˆå­£ï¼‰'
            instance_agg_label = 'å®ä¾‹å­£ç»Ÿè®¡æ•°é‡'
            database_agg_label = 'æ•°æ®åº“å­£ç»Ÿè®¡æ•°é‡'
        else:
            instance_label = 'å®ä¾‹æ•°æ€»é‡'
            database_label = 'æ•°æ®åº“æ•°æ€»é‡'
            instance_agg_label = 'å®ä¾‹ç»Ÿè®¡æ•°é‡'
            database_agg_label = 'æ•°æ®åº“ç»Ÿè®¡æ•°é‡'
        
        # æ„å»ºChart.jsæ•°æ®é›†
        datasets = [
            {
                'label': instance_label,
                'data': instance_count_data,
                'borderColor': '#FF6384',
                'backgroundColor': 'rgba(255, 99, 132, 0.1)',
                'borderWidth': 2,
                'pointStyle': 'circle',
                'tension': 0.1
            },
            {
                'label': database_label,
                'data': database_count_data,
                'borderColor': '#36A2EB',
                'backgroundColor': 'rgba(54, 162, 235, 0.1)',
                'borderWidth': 2,
                'pointStyle': 'rect',
                'tension': 0.1
            },
            {
                'label': instance_agg_label,
                'data': instance_aggregation_data,
                'borderColor': '#FFCE56',
                'backgroundColor': 'rgba(255, 206, 86, 0.1)',
                'borderWidth': 2,
                'pointStyle': 'triangle',
                'tension': 0.1,
                'borderDash': [5, 5]
            },
            {
                'label': database_agg_label,
                'data': database_aggregation_data,
                'borderColor': '#4BC0C0',
                'backgroundColor': 'rgba(75, 192, 192, 0.1)',
                'borderWidth': 2,
                'pointStyle': 'star',
                'tension': 0.1,
                'borderDash': [10, 5]
            }
        ]
        
        return jsonify({
            'success': True,
            'labels': labels,
            'datasets': datasets,
            'dataPointCount': len(labels),
            'timeRange': f'{start_date.strftime("%Y-%m-%d")} - {end_date.strftime("%Y-%m-%d")}',
            'yAxisLabel': 'æ•°é‡',
            'chartTitle': f'{period_type.title()}æ ¸å¿ƒæŒ‡æ ‡ç»Ÿè®¡',
            'periodType': period_type
        })
        
    except Exception as e:
        logger.error(f"è·å–æ ¸å¿ƒèšåˆæŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e),
            'labels': [],
            'datasets': [],
            'dataPointCount': 0,
            'timeRange': '',
            'yAxisLabel': 'æ•°é‡',
            'chartTitle': 'æ ¸å¿ƒæŒ‡æ ‡ç»Ÿè®¡'
        }), 500


@partition_bp.route('/api/aggregations/chart', methods=['GET'])
@login_required
@view_required
def get_aggregations_chart():
    """
    è·å–èšåˆæ•°æ®å›¾è¡¨æ•°æ®ï¼ˆæŒ‰å®ä¾‹åˆ†åˆ«æ˜¾ç¤ºï¼‰
    """
    try:
        from datetime import date, timedelta
        
        # è·å–æŸ¥è¯¢å‚æ•°
        period_type = request.args.get('period_type', 'daily')
        days = request.args.get('days', 7, type=int)
        
        # éªŒè¯å‘¨æœŸç±»å‹
        if period_type not in ['daily', 'weekly', 'monthly', 'quarterly']:
            return jsonify({'error': 'Invalid period_type'}), 400
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = date.today()
        start_date = end_date - timedelta(days=days)
        
        # ç®€åŒ–å®ç°ï¼šç›´æ¥æŸ¥è¯¢ä¸»è¡¨ï¼Œä¸ä½¿ç”¨å¤æ‚çš„UNIONæŸ¥è¯¢
        from app.models.database_size_aggregation import DatabaseSizeAggregation
        from app.models.instance_size_aggregation import InstanceSizeAggregation
        from app.models.database_size_stat import DatabaseSizeStat
        from app.models.instance_size_stat import InstanceSizeStat
        
        # æŸ¥è¯¢æ•°æ®åº“èšåˆæ•°æ®
        db_aggregations = DatabaseSizeAggregation.query.filter(
            DatabaseSizeAggregation.period_type == period_type,
            DatabaseSizeAggregation.period_start >= start_date,
            DatabaseSizeAggregation.period_start <= end_date
        ).join(Instance).all()
        
        # æŸ¥è¯¢å®ä¾‹èšåˆæ•°æ®
        instance_aggregations = InstanceSizeAggregation.query.filter(
            InstanceSizeAggregation.period_type == period_type,
            InstanceSizeAggregation.period_start >= start_date,
            InstanceSizeAggregation.period_start <= end_date
        ).join(Instance).all()
        
        # æŸ¥è¯¢æ•°æ®åº“ç»Ÿè®¡æ•°æ®ï¼ˆdailyç±»å‹ï¼‰
        db_stats = []
        if period_type == 'daily':
            db_stats = DatabaseSizeStat.query.filter(
                DatabaseSizeStat.collected_date >= start_date,
                DatabaseSizeStat.collected_date <= end_date
            ).join(Instance).all()
        
        # æŸ¥è¯¢å®ä¾‹ç»Ÿè®¡æ•°æ®ï¼ˆdailyç±»å‹ï¼‰
        instance_stats = []
        if period_type == 'daily':
            instance_stats = InstanceSizeStat.query.filter(
                InstanceSizeStat.collected_date >= start_date,
                InstanceSizeStat.collected_date <= end_date
            ).join(Instance).all()
        
        # å¤„ç†æ•°æ®ä¸ºChart.jsæ ¼å¼ - æ˜¾ç¤ºèšåˆæ•°æ®æ•°é‡ç»Ÿè®¡
        processed_data = {}
        
        # æŒ‰æ—¥æœŸç»Ÿè®¡èšåˆæ•°æ®æ•°é‡
        from collections import defaultdict
        daily_counts = defaultdict(lambda: defaultdict(int))  # {date: {instance: count}}
        
        # ç»Ÿè®¡æ•°æ®åº“èšåˆæ•°æ®æ•°é‡
        for agg in db_aggregations:
            period_start_str = agg.period_start.strftime('%Y-%m-%d')
            instance_name = agg.instance.name if agg.instance else 'Unknown'
            daily_counts[period_start_str][f"ğŸ“Š {instance_name} - æ•°æ®åº“èšåˆ"] += 1
        
        # ç»Ÿè®¡å®ä¾‹èšåˆæ•°æ®æ•°é‡
        for agg in instance_aggregations:
            period_start_str = agg.period_start.strftime('%Y-%m-%d')
            instance_name = agg.instance.name if agg.instance else 'Unknown'
            daily_counts[period_start_str][f"ğŸ–¥ï¸ {instance_name} - å®ä¾‹èšåˆ"] += 1
        
        # ç»Ÿè®¡æ•°æ®åº“ç»Ÿè®¡æ•°æ®æ•°é‡ï¼ˆdailyç±»å‹ï¼‰
        for stat in db_stats:
            period_start_str = stat.collected_date.strftime('%Y-%m-%d')
            instance_name = stat.instance.name if stat.instance else 'Unknown'
            daily_counts[period_start_str][f"ğŸ“ˆ {instance_name} - æ•°æ®åº“ç»Ÿè®¡"] += 1
        
        # ç»Ÿè®¡å®ä¾‹ç»Ÿè®¡æ•°æ®æ•°é‡ï¼ˆdailyç±»å‹ï¼‰
        for stat in instance_stats:
            period_start_str = stat.collected_date.strftime('%Y-%m-%d')
            instance_name = stat.instance.name if stat.instance else 'Unknown'
            daily_counts[period_start_str][f"ğŸ“ˆ {instance_name} - å®ä¾‹ç»Ÿè®¡"] += 1
        
        # è½¬æ¢ä¸ºå›¾è¡¨æ ¼å¼
        for date_str, instance_counts in daily_counts.items():
            for instance_key, count in instance_counts.items():
                if instance_key not in processed_data:
                    processed_data[instance_key] = {'labels': [], 'data': []}
                
                processed_data[instance_key]['labels'].append(date_str)
                processed_data[instance_key]['data'].append(count)

        # ç”Ÿæˆæ‰€æœ‰å”¯ä¸€çš„æ—¥æœŸæ ‡ç­¾
        all_labels = set()
        for key, value in processed_data.items():
            all_labels.update(value['labels'])
        labels = sorted(list(all_labels))
        
        # ç”ŸæˆChart.jsæ•°æ®é›†
        datasets = []
        import random
        for key, value in processed_data.items():
            # ç¡®ä¿æ•°æ®ç‚¹ä¸æ‰€æœ‰æ ‡ç­¾å¯¹é½ï¼Œç¼ºå¤±çš„æ—¥æœŸç”¨0å¡«å……
            aligned_data = []
            data_map = dict(zip(value['labels'], value['data']))
            for lbl in labels:
                aligned_data.append(data_map.get(lbl, 0))

            datasets.append({
                'label': key,
                'data': aligned_data,
                'fill': False,
                'borderColor': '#'+''.join([random.choice('0123456789ABCDEF') for j in range(6)]),
                'tension': 0.1
            })
        
        return jsonify({
                   'labels': labels,
                   'datasets': datasets,
                   'dataPointCount': len(labels),
                   'timeRange': f'{start_date.strftime("%Y-%m-%d")} - {end_date.strftime("%Y-%m-%d")}',
                   'yAxisLabel': 'èšåˆæ•°æ®æ•°é‡',
                   'chartTitle': f'{period_type.title()}èšåˆæ•°æ®ç»Ÿè®¡'
               })
        
    except Exception as e:
        logger.error(f"è·å–èšåˆæ•°æ®å›¾è¡¨æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        return jsonify({
            'labels': [],
            'datasets': [],
            'dataPointCount': 0,
            'timeRange': '',
            'error': f'Failed to load chart data: {str(e)}'
        }), 500


