"""
åˆ†åŒºç®¡ç† API è·¯ç”±
æä¾›æ•°æ®åº“è¡¨åˆ†åŒºåˆ›å»ºã€æ¸…ç†ã€ç»Ÿè®¡ç­‰åŠŸèƒ½
"""

import logging
from datetime import datetime
from flask import Blueprint, request, jsonify, render_template
from flask_login import login_required, current_user
from sqlalchemy import and_, or_, func, desc, text
from app.services.partition_management_service import PartitionManagementService
from app.tasks.partition_management_tasks import get_partition_management_status
from app.tasks.database_size_aggregation_tasks import cleanup_old_aggregations
from app.models.instance import Instance
from app.models.database_size_aggregation import DatabaseSizeAggregation
from app.models.database_size_stat import DatabaseSizeStat
from app.utils.decorators import view_required
from app import db

logger = logging.getLogger(__name__)

# åˆ›å»ºè“å›¾
partition_bp = Blueprint('partition', __name__)

# é¡µé¢è·¯ç”±
@partition_bp.route('/', methods=['GET'])
@login_required
@view_required
def partitions():
    """
    åˆ†åŒºç®¡ç†é¡µé¢æˆ–API
    
    å¦‚æžœæ˜¯é¡µé¢è¯·æ±‚ï¼ˆæ— æŸ¥è¯¢å‚æ•°ï¼‰ï¼Œè¿”å›žHTMLé¡µé¢
    å¦‚æžœæ˜¯APIè¯·æ±‚ï¼ˆæœ‰æŸ¥è¯¢å‚æ•°ï¼‰ï¼Œè¿”å›žJSONæ•°æ®
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰æŸ¥è¯¢å‚æ•°ï¼Œå¦‚æžœæœ‰åˆ™è¿”å›žAPIæ•°æ®
    if request.args:
        try:
            logger.info("å¼€å§‹èŽ·å–åˆ†åŒºä¿¡æ¯")
            service = PartitionManagementService()
            result = service.get_partition_info()
            logger.info(f"åˆ†åŒºä¿¡æ¯èŽ·å–ç»“æžœ: {result}")
            
            if result['success']:
                return jsonify({
                    'success': True,
                    'data': result,
                    'timestamp': datetime.utcnow().isoformat()
                })
            else:
                logger.error(f"åˆ†åŒºä¿¡æ¯èŽ·å–å¤±è´¥: {result}")
                return jsonify({
                    'success': False,
                    'error': result.get('message', 'èŽ·å–åˆ†åŒºä¿¡æ¯å¤±è´¥'),
                    'details': result
                }), 500
                
        except Exception as e:
            logger.error(f"èŽ·å–åˆ†åŒºä¿¡æ¯æ—¶å‡ºé”™: {str(e)}", exc_info=True)
            return jsonify({
                'success': False,
                'error': f'èŽ·å–åˆ†åŒºä¿¡æ¯å¤±è´¥: {str(e)}',
                'message': 'ç³»ç»Ÿå†…éƒ¨é”™è¯¯'
            }), 500
    else:
        # æ— æŸ¥è¯¢å‚æ•°ï¼Œè¿”å›žHTMLé¡µé¢
        return render_template('database_sizes/partitions.html')

# API è·¯ç”±

@partition_bp.route('/status', methods=['GET'])
@login_required
@view_required
def get_partition_status():
    """
    èŽ·å–åˆ†åŒºç®¡ç†çŠ¶æ€
    
    Returns:
        JSON: åˆ†åŒºçŠ¶æ€
    """
    try:
        result = get_partition_management_status()
        
        if result['success']:
            return jsonify({
                'success': True,
                'data': result,
                'timestamp': datetime.utcnow().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'error': result['message']
            }), 500
            
    except Exception as e:
        logger.error(f"èŽ·å–åˆ†åŒºçŠ¶æ€æ—¶å‡ºé”™: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500


@partition_bp.route('/test', methods=['GET'])
@login_required
@view_required
def test_partition_service():
    """
    æµ‹è¯•åˆ†åŒºç®¡ç†æœåŠ¡ï¼ˆè°ƒè¯•ç”¨ï¼‰
    
    Returns:
        JSON: æµ‹è¯•ç»“æžœ
    """
    try:
        logger.info("å¼€å§‹æµ‹è¯•åˆ†åŒºç®¡ç†æœåŠ¡")
        
        # æµ‹è¯•æ•°æ®åº“è¿žæŽ¥
        try:
            db.session.execute(text("SELECT 1"))
            logger.info("æ•°æ®åº“è¿žæŽ¥æ­£å¸¸")
        except Exception as db_error:
            logger.error(f"æ•°æ®åº“è¿žæŽ¥å¤±è´¥: {str(db_error)}")
            return jsonify({
                'success': False,
                'error': f'æ•°æ®åº“è¿žæŽ¥å¤±è´¥: {str(db_error)}'
            }), 500
        
        # æµ‹è¯•åˆ†åŒºç®¡ç†æœåŠ¡
        service = PartitionManagementService()
        result = service.get_partition_info()
        
        return jsonify({
            'success': True,
            'data': result,
            'timestamp': datetime.utcnow().isoformat()
        })
        
    except Exception as e:
        logger.error(f"æµ‹è¯•åˆ†åŒºç®¡ç†æœåŠ¡æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        return jsonify({
            'success': False,
            'error': f'æµ‹è¯•å¤±è´¥: {str(e)}'
        }), 500


@partition_bp.route('/create', methods=['POST'])
@login_required
@view_required
def create_partition():
    """
    åˆ›å»ºåˆ†åŒº
    
    Returns:
        JSON: åˆ›å»ºç»“æžœ
    """
    try:
        data = request.get_json()
        partition_date_str = data.get('date')
        
        if not partition_date_str:
            return jsonify({'error': 'ç¼ºå°‘æ—¥æœŸå‚æ•°'}), 400
        
        # è§£æžæ—¥æœŸ
        try:
            from datetime import datetime
            partition_date = datetime.strptime(partition_date_str, '%Y-%m-%d').date()
        except ValueError:
            return jsonify({'error': 'æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼'}), 400
        
        service = PartitionManagementService()
        result = service.create_partition(partition_date)
        
        if result['success']:
            return jsonify({
                'success': True,
                'data': result,
                'timestamp': datetime.utcnow().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'error': result['message']
            }), 500
            
    except Exception as e:
        logger.error(f"åˆ›å»ºåˆ†åŒºæ—¶å‡ºé”™: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500


@partition_bp.route('/cleanup', methods=['POST'])
@login_required
@view_required
def cleanup_partitions():
    """
    æ¸…ç†æ—§åˆ†åŒº
    
    Returns:
        JSON: æ¸…ç†ç»“æžœ
    """
    try:
        data = request.get_json() or {}
        retention_months = data.get('retention_months', 12)
        
        service = PartitionManagementService()
        result = service.cleanup_old_partitions(retention_months=retention_months)
        
        if result['success']:
            return jsonify({
                'success': True,
                'data': result,
                'timestamp': datetime.utcnow().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'error': result['message']
            }), 500
            
    except Exception as e:
        logger.error(f"æ¸…ç†åˆ†åŒºæ—¶å‡ºé”™: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500


@partition_bp.route('/statistics', methods=['GET'])
@login_required
@view_required
def get_partition_statistics():
    """
    èŽ·å–åˆ†åŒºç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        JSON: ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        service = PartitionManagementService()
        result = service.get_partition_statistics()
        
        if result['success']:
            return jsonify({
                'success': True,
                'data': result,
                'timestamp': datetime.utcnow().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'error': result['message']
            }), 500
            
    except Exception as e:
        logger.error(f"èŽ·å–åˆ†åŒºç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500


@partition_bp.route('/create-future', methods=['POST'])
@login_required
@view_required
def create_future_partitions():
    """
    åˆ›å»ºæœªæ¥åˆ†åŒº
    
    Returns:
        JSON: åˆ›å»ºç»“æžœ
    """
    try:
        data = request.get_json() or {}
        months_ahead = data.get('months_ahead', 3)
        
        service = PartitionManagementService()
        result = service.create_future_partitions(months_ahead=months_ahead)
        
        if result['success']:
            return jsonify({
                'success': True,
                'data': result,
                'timestamp': datetime.utcnow().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'error': result['message']
            }), 500
            
    except Exception as e:
        logger.error(f"åˆ›å»ºæœªæ¥åˆ†åŒºæ—¶å‡ºé”™: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500


@partition_bp.route('/aggregations/latest', methods=['GET'])
@login_required
@view_required
def get_latest_aggregations():
    """
    èŽ·å–æœ€æ–°çš„æ—¥ã€å‘¨ã€æœˆã€å­£åº¦èšåˆæ•°æ®ï¼ˆç”¨äºŽå¿«é€Ÿæ£€æŸ¥å¼‚å¸¸ï¼‰
    """
    try:
        # èŽ·å–æŸ¥è¯¢å‚æ•°
        instance_id = request.args.get('instance_id', type=int)
        db_type = request.args.get('db_type')
        database_name = request.args.get('database_name')
        
        # æž„å»ºæŸ¥è¯¢
        query = DatabaseSizeAggregation.query.join(Instance)
        
        # è¿‡æ»¤æŽ‰å·²åˆ é™¤çš„æ•°æ®åº“
        latest_stats_subquery = db.session.query(
            DatabaseSizeStat.instance_id,
            DatabaseSizeStat.database_name,
            DatabaseSizeStat.is_deleted,
            func.row_number().over(
                partition_by=[DatabaseSizeStat.instance_id, DatabaseSizeStat.database_name],
                order_by=DatabaseSizeStat.collected_date.desc()
            ).label('rn')
        ).subquery()
        
        active_databases_subquery = db.session.query(
            latest_stats_subquery.c.instance_id,
            latest_stats_subquery.c.database_name
        ).filter(
            and_(
                latest_stats_subquery.c.rn == 1,
                or_(
                    latest_stats_subquery.c.is_deleted == False,
                    latest_stats_subquery.c.is_deleted.is_(None)
                )
            )
        ).subquery()
        
        query = query.join(
            active_databases_subquery,
            and_(
                DatabaseSizeAggregation.instance_id == active_databases_subquery.c.instance_id,
                DatabaseSizeAggregation.database_name == active_databases_subquery.c.database_name
            )
        )
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        if instance_id:
            query = query.filter(DatabaseSizeAggregation.instance_id == instance_id)
        if db_type:
            query = query.filter(Instance.db_type == db_type)
        if database_name:
            query = query.filter(DatabaseSizeAggregation.database_name == database_name)
        
        # èŽ·å–æ¯ç§å‘¨æœŸç±»åž‹çš„æœ€æ–°æ•°æ®
        latest_data = {}
        period_types = ['daily', 'weekly', 'monthly', 'quarterly']
        
        for period_type in period_types:
            # èŽ·å–è¯¥å‘¨æœŸç±»åž‹çš„æœ€æ–°æ•°æ®
            period_query = query.filter(DatabaseSizeAggregation.period_type == period_type)
            period_query = period_query.order_by(desc(DatabaseSizeAggregation.period_start))
            
            # èŽ·å–æ¯ä¸ªå®žä¾‹-æ•°æ®åº“ç»„åˆçš„æœ€æ–°è®°å½•
            latest_period_data = {}
            for agg in period_query.all():
                key = f"{agg.instance.name}_{agg.database_name}"
                if key not in latest_period_data:
                    latest_period_data[key] = {
                        'instance': {
                            'id': agg.instance.id,
                            'name': agg.instance.name,
                            'db_type': agg.instance.db_type
                        },
                        'database_name': agg.database_name,
                        'period_type': agg.period_type,
                        'period_start': agg.period_start.isoformat(),
                        'period_end': agg.period_end.isoformat(),
                        'max_size_mb': agg.max_size_mb,
                        'min_size_mb': agg.min_size_mb,
                        'avg_size_mb': agg.avg_size_mb,
                        'data_count': agg.data_count,
                        'calculated_at': agg.calculated_at.isoformat() if agg.calculated_at else None
                    }
            
            latest_data[period_type] = list(latest_period_data.values())
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_records = sum(len(data) for data in latest_data.values())
        
        return jsonify({
            'success': True,
            'data': latest_data,
            'total_records': total_records,
            'period_types': period_types,
            'summary': {
                'daily': len(latest_data.get('daily', [])),
                'weekly': len(latest_data.get('weekly', [])),
                'monthly': len(latest_data.get('monthly', [])),
                'quarterly': len(latest_data.get('quarterly', []))
            }
        })
        
    except Exception as e:
        logger.error(f"èŽ·å–æœ€æ–°èšåˆæ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@partition_bp.route('/aggregations/cleanup', methods=['POST'])
@login_required
@view_required
def cleanup_old_aggregations_api():
    """
    æ¸…ç†æ—§çš„èšåˆæ•°æ®
    
    Returns:
        JSON: æ¸…ç†ç»“æžœ
    """
    try:
        data = request.get_json() or {}
        retention_days = data.get('retention_days', 365)
        
        result = cleanup_old_aggregations(retention_days=retention_days)
        
        if result['success']:
            return jsonify({
                'success': True,
                'data': result,
                'timestamp': datetime.utcnow().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'error': result['message']
            }), 500
            
    except Exception as e:
        logger.error(f"æ¸…ç†æ—§èšåˆæ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500


@partition_bp.route('/api/aggregations/summary', methods=['GET'])
@login_required
@view_required
def get_aggregations_summary():
    """
    èŽ·å–èšåˆæ•°æ®ç»Ÿè®¡æ¦‚è§ˆ
    """
    try:
        # èŽ·å–æ¯ç§å‘¨æœŸç±»åž‹çš„è®°å½•æ•°
        daily_count = DatabaseSizeAggregation.query.filter(
            DatabaseSizeAggregation.period_type == 'daily'
        ).count()
        
        weekly_count = DatabaseSizeAggregation.query.filter(
            DatabaseSizeAggregation.period_type == 'weekly'
        ).count()
        
        monthly_count = DatabaseSizeAggregation.query.filter(
            DatabaseSizeAggregation.period_type == 'monthly'
        ).count()
        
        quarterly_count = DatabaseSizeAggregation.query.filter(
            DatabaseSizeAggregation.period_type == 'quarterly'
        ).count()
        
        return jsonify({
            'success': True,
            'daily': daily_count,
            'weekly': weekly_count,
            'monthly': monthly_count,
            'quarterly': quarterly_count
        })
        
    except Exception as e:
        logger.error(f"èŽ·å–èšåˆæ•°æ®æ¦‚è§ˆæ—¶å‡ºé”™: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@partition_bp.route('/api/aggregations/chart', methods=['GET'])
@login_required
@view_required
def get_aggregations_chart():
    """
    èŽ·å–èšåˆæ•°æ®å›¾è¡¨æ•°æ®
    """
    try:
        from datetime import date, timedelta
        
        # èŽ·å–æŸ¥è¯¢å‚æ•°
        period_type = request.args.get('period_type', 'daily')
        days = request.args.get('days', 7, type=int)
        
        # éªŒè¯å‘¨æœŸç±»åž‹
        if period_type not in ['daily', 'weekly', 'monthly', 'quarterly']:
            return jsonify({'error': 'Invalid period_type'}), 400
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = date.today()
        start_date = end_date - timedelta(days=days)
        
        # ç®€åŒ–å®žçŽ°ï¼šç›´æŽ¥æŸ¥è¯¢ä¸»è¡¨ï¼Œä¸ä½¿ç”¨å¤æ‚çš„UNIONæŸ¥è¯¢
        from app.models.database_size_aggregation import DatabaseSizeAggregation
        from app.models.instance_size_aggregation import InstanceSizeAggregation
        from app.models.database_size_stat import DatabaseSizeStat
        from app.models.instance_size_stat import InstanceSizeStat
        
        # æŸ¥è¯¢æ•°æ®åº“èšåˆæ•°æ®
        db_aggregations = DatabaseSizeAggregation.query.filter(
            DatabaseSizeAggregation.period_type == period_type,
            DatabaseSizeAggregation.period_start >= start_date,
            DatabaseSizeAggregation.period_start <= end_date
        ).join(Instance).all()
        
        # æŸ¥è¯¢å®žä¾‹èšåˆæ•°æ®
        instance_aggregations = InstanceSizeAggregation.query.filter(
            InstanceSizeAggregation.period_type == period_type,
            InstanceSizeAggregation.period_start >= start_date,
            InstanceSizeAggregation.period_start <= end_date
        ).join(Instance).all()
        
        # æŸ¥è¯¢æ•°æ®åº“ç»Ÿè®¡æ•°æ®ï¼ˆdailyç±»åž‹ï¼‰
        db_stats = []
        if period_type == 'daily':
            db_stats = DatabaseSizeStat.query.filter(
                DatabaseSizeStat.collected_date >= start_date,
                DatabaseSizeStat.collected_date <= end_date
            ).join(Instance).all()
        
        # æŸ¥è¯¢å®žä¾‹ç»Ÿè®¡æ•°æ®ï¼ˆdailyç±»åž‹ï¼‰
        instance_stats = []
        if period_type == 'daily':
            instance_stats = InstanceSizeStat.query.filter(
                InstanceSizeStat.collected_date >= start_date,
                InstanceSizeStat.collected_date <= end_date
            ).join(Instance).all()
        
        # å¤„ç†æ•°æ®ä¸ºChart.jsæ ¼å¼ - æ˜¾ç¤ºèšåˆæ•°æ®æ•°é‡ç»Ÿè®¡
        processed_data = {}
        
        # æŒ‰æ—¥æœŸç»Ÿè®¡èšåˆæ•°æ®æ•°é‡
        from collections import defaultdict
        daily_counts = defaultdict(lambda: defaultdict(int))  # {date: {instance: count}}
        
        # ç»Ÿè®¡æ•°æ®åº“èšåˆæ•°æ®æ•°é‡
        for agg in db_aggregations:
            period_start_str = agg.period_start.strftime('%Y-%m-%d')
            instance_name = agg.instance.name if agg.instance else 'Unknown'
            daily_counts[period_start_str][f"ðŸ“Š {instance_name} - æ•°æ®åº“èšåˆ"] += 1
        
        # ç»Ÿè®¡å®žä¾‹èšåˆæ•°æ®æ•°é‡
        for agg in instance_aggregations:
            period_start_str = agg.period_start.strftime('%Y-%m-%d')
            instance_name = agg.instance.name if agg.instance else 'Unknown'
            daily_counts[period_start_str][f"ðŸ–¥ï¸ {instance_name} - å®žä¾‹èšåˆ"] += 1
        
        # ç»Ÿè®¡æ•°æ®åº“ç»Ÿè®¡æ•°æ®æ•°é‡ï¼ˆdailyç±»åž‹ï¼‰
        for stat in db_stats:
            period_start_str = stat.collected_date.strftime('%Y-%m-%d')
            instance_name = stat.instance.name if stat.instance else 'Unknown'
            daily_counts[period_start_str][f"ðŸ“ˆ {instance_name} - æ•°æ®åº“ç»Ÿè®¡"] += 1
        
        # ç»Ÿè®¡å®žä¾‹ç»Ÿè®¡æ•°æ®æ•°é‡ï¼ˆdailyç±»åž‹ï¼‰
        for stat in instance_stats:
            period_start_str = stat.collected_date.strftime('%Y-%m-%d')
            instance_name = stat.instance.name if stat.instance else 'Unknown'
            daily_counts[period_start_str][f"ðŸ“ˆ {instance_name} - å®žä¾‹ç»Ÿè®¡"] += 1
        
        # è½¬æ¢ä¸ºå›¾è¡¨æ ¼å¼
        for date_str, instance_counts in daily_counts.items():
            for instance_key, count in instance_counts.items():
                if instance_key not in processed_data:
                    processed_data[instance_key] = {'labels': [], 'data': []}
                
                processed_data[instance_key]['labels'].append(date_str)
                processed_data[instance_key]['data'].append(count)

        # ç”Ÿæˆæ‰€æœ‰å”¯ä¸€çš„æ—¥æœŸæ ‡ç­¾
        all_labels = set()
        for key, value in processed_data.items():
            all_labels.update(value['labels'])
        labels = sorted(list(all_labels))
        
        # ç”ŸæˆChart.jsæ•°æ®é›†
        datasets = []
        import random
        for key, value in processed_data.items():
            # ç¡®ä¿æ•°æ®ç‚¹ä¸Žæ‰€æœ‰æ ‡ç­¾å¯¹é½ï¼Œç¼ºå¤±çš„æ—¥æœŸç”¨0å¡«å……
            aligned_data = []
            data_map = dict(zip(value['labels'], value['data']))
            for lbl in labels:
                aligned_data.append(data_map.get(lbl, 0))

            datasets.append({
                'label': key,
                'data': aligned_data,
                'fill': False,
                'borderColor': '#'+''.join([random.choice('0123456789ABCDEF') for j in range(6)]),
                'tension': 0.1
            })
        
        return jsonify({
                   'labels': labels,
                   'datasets': datasets,
                   'dataPointCount': len(labels),
                   'timeRange': f'{start_date.strftime("%Y-%m-%d")} - {end_date.strftime("%Y-%m-%d")}',
                   'yAxisLabel': 'èšåˆæ•°æ®æ•°é‡',
                   'chartTitle': f'{period_type.title()}èšåˆæ•°æ®ç»Ÿè®¡'
               })
        
    except Exception as e:
        logger.error(f"èŽ·å–èšåˆæ•°æ®å›¾è¡¨æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        return jsonify({
            'labels': [],
            'datasets': [],
            'dataPointCount': 0,
            'timeRange': '',
            'error': f'Failed to load chart data: {str(e)}'
        }), 500


